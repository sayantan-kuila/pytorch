## Changes in number of hidden layers

**Activation Function:** ReLu  
**Loss Function:** crossentropy  
**Optimizer:** Adam  
**Batch size:** 64

| Number of Hidden Layers | Epoch | Training Accuracy | Testing Accuracy | Training Loss | Testing Loss |
|-------------------------|-------|-------------------|------------------|--------------|--------------|
| 2                       | 1     | 0.90              | 0.95             | 0.36         | 0.18         |
| 2                       | 2     | 0.95              | 0.96             | 0.16         | 0.13         |
| 2                       | 3     | 0.96              | 0.97             | 0.11         | 0.11         |
| 2                       | 4     | 0.97              | 0.97             | 0.09         | 0.11         |
| 2                       | 5     | 0.98              | 0.97             | 0.07         | 0.09         |
| 2                       | 6     | 0.98              | 0.97             | 0.06         | 0.09         |
| 2                       | 7     | 0.98              | 0.97             | 0.05         | 0.10         |
| 2                       | 8     | 0.99              | 0.97             | 0.04         | 0.10         |
| 2                       | 9     | 0.99              | 0.97             | 0.04         | 0.09         |
| 2                       | 10    | 0.99              | 0.98             | 0.03         | 0.09         |
| 5                       | 1     | 0.86              | 0.93             | 0.45         | 0.22         |
| 5                       | 2     | 0.94              | 0.95             | 0.18         | 0.17         |
| 5                       | 3     | 0.96              | 0.96             | 0.14         | 0.13         |
| 5                       | 4     | 0.97              | 0.96             | 0.11         | 0.12         |
| 5                       | 5     | 0.97              | 0.97             | 0.10         | 0.11         |
| 5                       | 6     | 0.97              | 0.96             | 0.09         | 0.14         |
| 5                       | 7     | 0.98              | 0.96             | 0.08         | 0.12         |
| 5                       | 8     | 0.98              | 0.97             | 0.07         | 0.11         |
| 5                       | 9     | 0.98              | 0.97             | 0.06         | 0.11         |
| 5                       | 10    | 0.98              | 0.97             | 0.06         | 0.10         |
| 10                      | 1     | 0.75              | 0.93             | 0.69         | 0.25         |
| 10                      | 2     | 0.94              | 0.96             | 0.22         | 0.16         |
| 10                      | 3     | 0.96              | 0.95             | 0.16         | 0.18         |
| 10                      | 4     | 0.96              | 0.97             | 0.14         | 0.13         |
| 10                      | 5     | 0.97              | 0.96             | 0.12         | 0.15         |
| 10                      | 6     | 0.97              | 0.97             | 0.11         | 0.15         |
| 10                      | 7     | 0.98              | 0.97             | 0.09         | 0.12         |
| 10                      | 8     | 0.98              | 0.96             | 0.08         | 0.14         |
| 10                      | 9     | 0.98              | 0.97             | 0.08         | 0.13         |
| 10                      | 10    | 0.98              | 0.97             | 0.07         | 0.14         |
| 13                      | 1     | 0.68              | 0.89             | 0.86         | 0.38         |
| 13                      | 2     | 0.93              | 0.95             | 0.26         | 0.19         |
| 13                      | 3     | 0.95              | 0.84             | 0.19         | 0.56         |
| 13                      | 4     | 0.93              | 0.90             | 0.23         | 0.35         |
| 13                      | 5     | 0.95              | 0.95             | 0.18         | 0.20         |
| 13                      | 6     | 0.96              | 0.96             | 0.15         | 0.17         |
| 13                      | 7     | 0.97              | 0.96             | 0.13         | 0.15         |
| 13                      | 8     | 0.97              | 0.95             | 0.12         | 0.20         |
| 13                      | 9     | 0.97              | 0.96             | 0.13         | 0.15         |
| 13                      | 10    | 0.97              | 0.97             | 0.12         | 0.14         |


## Training vs Testing Loss

Number of hidden layers: 2

![image](https://github.com/user-attachments/assets/a0cf9f9b-37c1-41d3-9beb-b5bbe3d15a6c)

Number of hidden layers: 5

![image](https://github.com/user-attachments/assets/c5963c68-1345-47f5-a026-6d12412f05a8)

Number of hidden layers: 10

<img width="570" alt="image" src="https://github.com/user-attachments/assets/36e03768-9b90-40c2-827e-890b3058888f" />

Number of hidden layers: 13

![image](https://github.com/user-attachments/assets/18ff4073-5000-4322-8418-d5b369e46b68)


